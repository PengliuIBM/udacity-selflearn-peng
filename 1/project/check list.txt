lesson2 project check list.txt
In this project, you will build a neural network of your own design to evaluate the MNIST dataset.
Some of the benchmark results on MNIST include can be found on Yann LeCun's page and include:
88% Lecun et al., 1998
95.3% Lecun et al., 1998
99.65% Ciresan et al., 2011
============
我有一个外卖点餐平台，partner完成了初始版本的开发。 后续工作主要包括：交付交接（代码级）；后续找人继续开发迭代；后续的运维和SRE；对于应用本身、中间件、infrastructure几个方面需要考虑和规划的提纲请列出。
###1. load dataset from torch.torchvision.datasets
import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision import datasets
import matplotlib.pyplot as plt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

###2. use transform or other Pytorch methods to convert the data to tensors, Normalize and flatten the data.

###3. create a DataLoader for my dataset
# Establish our transform
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

# Load train and test datasets
training_data = datasets.CIFAR10(root="data", train=True, download=True, transform=transform)
test_data = datasets.CIFAR10(root="data", train=False, download=True, transform=transform)

# Create the training and test dataloaders with a batch size of 32
train_loader = DataLoader(training_data, batch_size=32, shuffle=True)
test_loader = DataLoader(test_data, batch_size=32)

###4. Virtualized the dataset using provided function.
# This function will show us 5 images from our dataloader.
# You may not want to modify this function, or it will not work.
def show5(img_loader):
    # The 10 classes in the dataset
    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
    dataiter = iter(img_loader)

    batch = next(dataiter)
    labels = batch[1][0:5]
    images = batch[0][0:5]
    for i in range(5):
        print(classes[labels[i]])

        image = images[i].numpy()
        plt.imshow(image.transpose(1,2,0))
        plt.show()


# Create the training dataset
training_data = datasets.CIFAR10(root="data", train=True, download=True, transform=ToTensor())

# Create the training dataloader with batch size 5
train_loader = DataLoader(training_data, batch_size=5)

# View 5 images using the show5 function
show5(train_loader)
###5. used either my training data and inverting any normalization and flattening OR a second DataLoader without any normalization and flattening.
###6. Provided a brief adjusticifation of any necessary proprocessing steps or why no preprocessing is needed.
###7. Used Pytorch to build a neural network to predict the class of each given input image
# Define the class for your neural network
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.activation = F.relu
        self.fc1 = nn.Linear(32 * 32 * 3, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.fc3(x)
        return x

# Instantiate the model
net = Net()
net.to(device)

# Choose an optimizer
optimizer = optim.Adam(net.parameters(), lr=0.001)
# Choose a loss function
criterion = nn.CrossEntropyLoss()

############# train loop with optimizer and loss function.
num_epochs = 10

# Establish a list for our history
train_loss_history = list()
val_loss_history = list()

for epoch in range(num_epochs):
    net.train()
    train_loss = 0.0
    train_correct = 0
    for i, data in enumerate(train_loader):
        # data is a list of [inputs, labels] within 1 batch, size =32
        inputs, labels = data

        # Pass to GPU if available.  32inputs and 32 labels.
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        _, preds = torch.max(outputs.data, 1)
        train_correct += (preds == labels).float().mean().item()
        train_loss += loss.item()
    print(f'Epoch {epoch + 1} training accuracy: {train_correct/len(train_loader):.2f}% training loss: {train_loss/len(train_loader):.5f}')
    train_loss_history.append(train_loss/len(train_loader))


    val_loss = 0.0
    val_correct = 0
    net.eval()
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        outputs = net(inputs)
        loss = criterion(outputs, labels)

        _, preds = torch.max(outputs.data, 1)
        val_correct += (preds == labels).float().mean().item()
        val_loss += loss.item()
    print(f'Epoch {epoch + 1} validation accuracy: {val_correct/len(test_loader):.2f}% validation loss: {val_loss/len(test_loader):.5f}')
    val_loss_history.append(val_loss/len(test_loader))

# Plot the training and validation loss history
plt.plot(train_loss_history, label="Training Loss")
plt.plot(val_loss_history, label="Validation Loss")
plt.legend()
plt.show()

===========
